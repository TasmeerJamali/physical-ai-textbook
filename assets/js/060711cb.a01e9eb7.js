"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[803],{5401:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/fundamentals","title":"Chapter 4.1: Multimodal AI Fundamentals","description":"Vision-Language-Action (VLA) models combine multiple AI modalities to create truly intelligent robots. Let\'s understand how they work.","source":"@site/docs/module-4-vla/fundamentals.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/fundamentals","permalink":"/physical-ai-textbook/docs/module-4-vla/fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/TasmeerJamali/physical-ai-textbook/tree/main/docs/module-4-vla/fundamentals.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"textbookSidebar","previous":{"title":"Introduction to VLA Models","permalink":"/physical-ai-textbook/docs/module-4-vla/intro"}}');var a=t(4848),i=t(8453),s=t(6819);const l={sidebar_position:2},r="Chapter 4.1: Multimodal AI Fundamentals",d={},c=[{value:"\ud83e\udde0 What is Multimodal AI?",id:"-what-is-multimodal-ai",level:2},{value:"\ud83d\udd04 The VLA Pipeline",id:"-the-vla-pipeline",level:2},{value:"\ud83d\udd27 Key Components",id:"-key-components",level:2},{value:"1. Vision Encoder",id:"1-vision-encoder",level:3},{value:"2. Language Model",id:"2-language-model",level:3},{value:"3. Action Decoder",id:"3-action-decoder",level:3},{value:"\ud83d\udcca Popular VLA Models",id:"-popular-vla-models",level:2},{value:"\ud83d\udca1 Example: Pick and Place",id:"-example-pick-and-place",level:2},{value:"\u2705 Key Takeaways",id:"-key-takeaways",level:2},{value:"\u27a1\ufe0f Next Chapter",id:"\ufe0f-next-chapter",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-41-multimodal-ai-fundamentals",children:"Chapter 4.1: Multimodal AI Fundamentals"})}),"\n",(0,a.jsx)(s.A,{chapterId:"module-4-vla-fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"Vision-Language-Action (VLA) models combine multiple AI modalities to create truly intelligent robots. Let's understand how they work."}),"\n",(0,a.jsx)(n.h2,{id:"-what-is-multimodal-ai",children:"\ud83e\udde0 What is Multimodal AI?"}),"\n",(0,a.jsx)(n.p,{children:"Traditional AI models work with one type of data:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Vision models"}),": Process images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Language models"}),": Process text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Control models"}),": Generate actions"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Multimodal AI"})," combines these into unified systems."]}),"\n",(0,a.jsx)(n.h2,{id:"-the-vla-pipeline",children:"\ud83d\udd04 The VLA Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA Pipeline                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Camera  \u2502\u2500\u2500\u2500\u25b6\u2502 Vision  \u2502\u2500\u2500\u2500\u25b6\u2502  LLM    \u2502\u2500\u2500\u2500\u25b6\u2502Action \u2502 \u2502\n\u2502  \u2502         \u2502    \u2502 Encoder \u2502    \u2502 Decoder \u2502    \u2502Output \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502              \u2502              \u2502              \u2502     \u2502\n\u2502       \u25bc              \u25bc              \u25bc              \u25bc     \u2502\n\u2502    Image         Features       Reasoning      Robot    \u2502\n\u2502    Input         Extracted      + Planning     Commands \u2502\n\u2502                                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h2,{id:"-key-components",children:"\ud83d\udd27 Key Components"}),"\n",(0,a.jsx)(n.h3,{id:"1-vision-encoder",children:"1. Vision Encoder"}),"\n",(0,a.jsx)(n.p,{children:"Converts images to feature vectors:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n# Encode image\ninputs = processor(images=image, return_tensors="pt")\nfeatures = model.get_image_features(**inputs)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-language-model",children:"2. Language Model"}),"\n",(0,a.jsx)(n.p,{children:"Processes text and reasons about tasks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model="gpt-4-vision-preview",\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "What object should the robot pick up?"},\n                {"type": "image_url", "image_url": {"url": image_url}}\n            ]\n        }\n    ]\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-action-decoder",children:"3. Action Decoder"}),"\n",(0,a.jsx)(n.p,{children:"Converts reasoning to robot commands:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def decode_action(llm_output: str) -> dict:\n    """Convert LLM output to robot action."""\n    # Parse structured output\n    if "pick up" in llm_output.lower():\n        return {"action": "grasp", "target": extract_target(llm_output)}\n    elif "move to" in llm_output.lower():\n        return {"action": "navigate", "goal": extract_location(llm_output)}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-popular-vla-models",children:"\ud83d\udcca Popular VLA Models"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Model"}),(0,a.jsx)(n.th,{children:"Developer"}),(0,a.jsx)(n.th,{children:"Key Feature"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"RT-2"})}),(0,a.jsx)(n.td,{children:"Google"}),(0,a.jsx)(n.td,{children:"End-to-end vision-to-action"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"PaLM-E"})}),(0,a.jsx)(n.td,{children:"Google"}),(0,a.jsx)(n.td,{children:"Embodied language model"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"GPT-4V"})}),(0,a.jsx)(n.td,{children:"OpenAI"}),(0,a.jsx)(n.td,{children:"Vision + reasoning"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"LLaVA"})}),(0,a.jsx)(n.td,{children:"Open Source"}),(0,a.jsx)(n.td,{children:"Efficient multimodal"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"-example-pick-and-place",children:"\ud83d\udca1 Example: Pick and Place"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Complete VLA pipeline example\ndef pick_and_place(image, instruction):\n    # 1. Vision: Detect objects\n    objects = detect_objects(image)\n    \n    # 2. Language: Understand instruction\n    target = understand_instruction(instruction, objects)\n    \n    # 3. Action: Generate robot commands\n    commands = plan_grasp(target)\n    \n    return commands\n\n# Usage\ncommands = pick_and_place(\n    camera.capture(),\n    "Pick up the red cup and place it on the table"\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-key-takeaways",children:"\u2705 Key Takeaways"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"VLA combines vision, language, and action"}),"\n",(0,a.jsx)(n.li,{children:"Vision encoders extract image features"}),"\n",(0,a.jsx)(n.li,{children:"LLMs provide reasoning and planning"}),"\n",(0,a.jsx)(n.li,{children:"Action decoders generate robot commands"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-next-chapter",children:"\u27a1\ufe0f Next Chapter"}),"\n",(0,a.jsxs)(n.p,{children:["Continue to ",(0,a.jsx)(n.a,{href:"./voice-commands",children:"Chapter 4.2: Voice Commands with Whisper"})," to add speech input!"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},6819:(e,n,t)=>{t.d(n,{A:()=>s});var o=t(6540);const a={container:"container_Bg0B",buttonGroup:"buttonGroup_JAA4",loadingMessage:"loadingMessage_hq4B",pulse:"pulse_Ml0H",button:"button_uKyo",active:"active_rj2W",activeUrdu:"activeUrdu_sPXe"};var i=t(4848);function s({chapterId:e,apiUrl:n="http://localhost:8000"}){const[t,s]=(0,o.useState)(!1),[l,r]=(0,o.useState)(!1),[d,c]=(0,o.useState)(!1),[u,h]=(0,o.useState)(null),[p,m]=(0,o.useState)(null),[x,g]=(0,o.useState)(null),j=()=>{const e=document.querySelector("article");return e?.innerHTML||""};return(0,i.jsxs)("div",{className:a.container,children:[(t||l)&&(0,i.jsxs)("div",{className:a.loadingMessage,children:["\u23f3 ",t?"Personalizing":"Translating","... This may take 20-30 seconds"]}),(0,i.jsxs)("div",{className:a.buttonGroup,children:[(0,i.jsxs)("button",{className:a.button,onClick:async()=>{s(!0);try{const t=j();x||g(t);const o=await fetch(`${n}/api/personalize/adapt`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:t,chapter_id:e,user_level:localStorage.getItem("userLevel")||"intermediate",user_background:JSON.parse(localStorage.getItem("userBackground")||"{}")})}),a=await o.json();h(a.personalized_content);const i=document.querySelector("article");i&&(i.innerHTML=a.personalized_content)}catch(t){console.error("Personalization failed:",t)}finally{s(!1)}},disabled:t||l,title:"Personalize content based on your experience level",children:[t?"\u23f3":"\u2728"," Personalize"]}),(0,i.jsxs)("button",{className:`${a.button} ${d?a.activeUrdu:""}`,onClick:async()=>{if(d){const e=document.querySelector("article");return e&&(u?e.innerHTML=u:x?e.innerHTML=x:window.location.reload(),e.dir="ltr"),void c(!1)}r(!0);try{const e=j();x||g(e);const t=await fetch(`${n}/api/personalize/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:e,target_language:"ur",preserve_code:!0})}),o=await t.json();m(o.translated_content);const a=document.querySelector("article");a&&(a.innerHTML=o.translated_content,a.dir="rtl"),c(!0)}catch(e){console.error("Translation failed:",e)}finally{r(!1)}},disabled:l||t,title:d?"Switch back to English":"Translate to Urdu",children:[l?"\u23f3":d?"\ud83d\udd19":"\ud83c\udf10"," ",d?"Back to English":"\u0627\u0631\u062f\u0648"]})]})]})}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var o=t(6540);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);