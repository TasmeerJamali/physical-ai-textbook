"use strict";(globalThis.webpackChunkdocs_site=globalThis.webpackChunkdocs_site||[]).push([[803],{5401:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-4-vla/fundamentals","title":"Chapter 4.1: Multimodal AI Fundamentals","description":"Vision-Language-Action (VLA) models combine multiple AI modalities to create truly intelligent robots. Let\'s understand how they work.","source":"@site/docs/module-4-vla/fundamentals.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/fundamentals","permalink":"/physical-ai-textbook/docs/module-4-vla/fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/TasmeerJamali/physical-ai-textbook/tree/main/docs/module-4-vla/fundamentals.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"textbookSidebar","previous":{"title":"Introduction to VLA Models","permalink":"/physical-ai-textbook/docs/module-4-vla/intro"}}');var o=t(4848),l=t(8453),i=t(6819);const s={sidebar_position:2},r="Chapter 4.1: Multimodal AI Fundamentals",c={},d=[{value:"\ud83e\udde0 What is Multimodal AI?",id:"-what-is-multimodal-ai",level:2},{value:"\ud83d\udd04 The VLA Pipeline",id:"-the-vla-pipeline",level:2},{value:"\ud83d\udd27 Key Components",id:"-key-components",level:2},{value:"1. Vision Encoder",id:"1-vision-encoder",level:3},{value:"2. Language Model",id:"2-language-model",level:3},{value:"3. Action Decoder",id:"3-action-decoder",level:3},{value:"\ud83d\udcca Popular VLA Models",id:"-popular-vla-models",level:2},{value:"\ud83d\udca1 Example: Pick and Place",id:"-example-pick-and-place",level:2},{value:"\u2705 Key Takeaways",id:"-key-takeaways",level:2},{value:"\u27a1\ufe0f Next Chapter",id:"\ufe0f-next-chapter",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-41-multimodal-ai-fundamentals",children:"Chapter 4.1: Multimodal AI Fundamentals"})}),"\n",(0,o.jsx)(i.A,{chapterId:"module-4-vla-fundamentals"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) models combine multiple AI modalities to create truly intelligent robots. Let's understand how they work."}),"\n",(0,o.jsx)(n.h2,{id:"-what-is-multimodal-ai",children:"\ud83e\udde0 What is Multimodal AI?"}),"\n",(0,o.jsx)(n.p,{children:"Traditional AI models work with one type of data:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision models"}),": Process images"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language models"}),": Process text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Control models"}),": Generate actions"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Multimodal AI"})," combines these into unified systems."]}),"\n",(0,o.jsx)(n.h2,{id:"-the-vla-pipeline",children:"\ud83d\udd04 The VLA Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    VLA Pipeline                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Camera  \u2502\u2500\u2500\u2500\u25b6\u2502 Vision  \u2502\u2500\u2500\u2500\u25b6\u2502  LLM    \u2502\u2500\u2500\u2500\u25b6\u2502Action \u2502 \u2502\n\u2502  \u2502         \u2502    \u2502 Encoder \u2502    \u2502 Decoder \u2502    \u2502Output \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502       \u2502              \u2502              \u2502              \u2502     \u2502\n\u2502       \u25bc              \u25bc              \u25bc              \u25bc     \u2502\n\u2502    Image         Features       Reasoning      Robot    \u2502\n\u2502    Input         Extracted      + Planning     Commands \u2502\n\u2502                                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h2,{id:"-key-components",children:"\ud83d\udd27 Key Components"}),"\n",(0,o.jsx)(n.h3,{id:"1-vision-encoder",children:"1. Vision Encoder"}),"\n",(0,o.jsx)(n.p,{children:"Converts images to feature vectors:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n# Encode image\ninputs = processor(images=image, return_tensors="pt")\nfeatures = model.get_image_features(**inputs)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-language-model",children:"2. Language Model"}),"\n",(0,o.jsx)(n.p,{children:"Processes text and reasons about tasks:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model="gpt-4-vision-preview",\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "What object should the robot pick up?"},\n                {"type": "image_url", "image_url": {"url": image_url}}\n            ]\n        }\n    ]\n)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-action-decoder",children:"3. Action Decoder"}),"\n",(0,o.jsx)(n.p,{children:"Converts reasoning to robot commands:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def decode_action(llm_output: str) -> dict:\n    """Convert LLM output to robot action."""\n    # Parse structured output\n    if "pick up" in llm_output.lower():\n        return {"action": "grasp", "target": extract_target(llm_output)}\n    elif "move to" in llm_output.lower():\n        return {"action": "navigate", "goal": extract_location(llm_output)}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-popular-vla-models",children:"\ud83d\udcca Popular VLA Models"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Model"}),(0,o.jsx)(n.th,{children:"Developer"}),(0,o.jsx)(n.th,{children:"Key Feature"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"RT-2"})}),(0,o.jsx)(n.td,{children:"Google"}),(0,o.jsx)(n.td,{children:"End-to-end vision-to-action"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"PaLM-E"})}),(0,o.jsx)(n.td,{children:"Google"}),(0,o.jsx)(n.td,{children:"Embodied language model"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"GPT-4V"})}),(0,o.jsx)(n.td,{children:"OpenAI"}),(0,o.jsx)(n.td,{children:"Vision + reasoning"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"LLaVA"})}),(0,o.jsx)(n.td,{children:"Open Source"}),(0,o.jsx)(n.td,{children:"Efficient multimodal"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"-example-pick-and-place",children:"\ud83d\udca1 Example: Pick and Place"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Complete VLA pipeline example\ndef pick_and_place(image, instruction):\n    # 1. Vision: Detect objects\n    objects = detect_objects(image)\n    \n    # 2. Language: Understand instruction\n    target = understand_instruction(instruction, objects)\n    \n    # 3. Action: Generate robot commands\n    commands = plan_grasp(target)\n    \n    return commands\n\n# Usage\ncommands = pick_and_place(\n    camera.capture(),\n    "Pick up the red cup and place it on the table"\n)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"-key-takeaways",children:"\u2705 Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"VLA combines vision, language, and action"}),"\n",(0,o.jsx)(n.li,{children:"Vision encoders extract image features"}),"\n",(0,o.jsx)(n.li,{children:"LLMs provide reasoning and planning"}),"\n",(0,o.jsx)(n.li,{children:"Action decoders generate robot commands"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"\ufe0f-next-chapter",children:"\u27a1\ufe0f Next Chapter"}),"\n",(0,o.jsxs)(n.p,{children:["Continue to ",(0,o.jsx)(n.a,{href:"./voice-commands",children:"Chapter 4.2: Voice Commands with Whisper"})," to add speech input!"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},6819:(e,n,t)=>{t.d(n,{A:()=>i});var a=t(6540);const o={container:"container_Bg0B",levelSelector:"levelSelector__bWR",levelLabel:"levelLabel_Dtdn",levelButtons:"levelButtons__ic_",levelButton:"levelButton__CKK",levelButtonActive:"levelButtonActive_MqUN",buttonGroup:"buttonGroup_JAA4",loadingMessage:"loadingMessage_hq4B",pulse:"pulse_Ml0H",button:"button_uKyo",active:"active_rj2W",activeUrdu:"activeUrdu_sPXe",activePersonalized:"activePersonalized_egrT"};var l=t(4848);function i({chapterId:e,apiUrl:n="http://localhost:8000"}){const[t,i]=(0,a.useState)(!1),[s,r]=(0,a.useState)(!1),[c,d]=(0,a.useState)(!1),[u,h]=(0,a.useState)(null),[p,m]=(0,a.useState)(null),[x,g]=(0,a.useState)(null),[v,j]=(0,a.useState)("intermediate"),[b,f]=(0,a.useState)(!1);(0,a.useEffect)(()=>{const e=localStorage.getItem("userLevel");e&&j(e)},[]);const y=()=>{const e=document.querySelector("article");return e?.innerHTML||""};return(0,l.jsxs)("div",{className:o.container,children:[(t||s)&&(0,l.jsxs)("div",{className:o.loadingMessage,children:["\u23f3 ",t?"Personalizing":"Translating","... This may take 20-30 seconds"]}),(0,l.jsxs)("div",{className:o.levelSelector,children:[(0,l.jsx)("span",{className:o.levelLabel,children:"Your Level:"}),(0,l.jsx)("div",{className:o.levelButtons,children:["beginner","intermediate","advanced"].map(e=>(0,l.jsxs)("button",{className:`${o.levelButton} ${v===e?o.levelButtonActive:""}`,onClick:()=>(e=>{j(e),localStorage.setItem("userLevel",e)})(e),disabled:t||s,children:["beginner"===e?"\ud83c\udf31":"intermediate"===e?"\ud83c\udf3f":"\ud83c\udf33"," ",e.charAt(0).toUpperCase()+e.slice(1)]},e))})]}),(0,l.jsxs)("div",{className:o.buttonGroup,children:[(0,l.jsxs)("button",{className:`${o.button} ${b?o.activePersonalized:""}`,onClick:async()=>{if(b){const e=document.querySelector("article");return e&&x&&(e.innerHTML=x),f(!1),void h(null)}i(!0);try{const t=y();x||g(t);const a=await fetch(`${n}/api/personalize/adapt`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:t,chapter_id:e,user_level:v,user_background:JSON.parse(localStorage.getItem("userBackground")||"{}")})}),o=await a.json();h(o.personalized_content),f(!0);const l=document.querySelector("article");l&&(l.innerHTML=o.personalized_content)}catch(t){console.error("Personalization failed:",t)}finally{i(!1)}},disabled:t||s,title:b?"Restore original content":"Personalize content based on your experience level",children:[t?"\u23f3":b?"\ud83d\udd19":"\u2728"," ",b?"Back to Original":"Personalize"]}),(0,l.jsxs)("button",{className:`${o.button} ${c?o.activeUrdu:""}`,onClick:async()=>{if(c){const e=document.querySelector("article");return e&&(u?e.innerHTML=u:x?e.innerHTML=x:window.location.reload(),e.dir="ltr"),void d(!1)}r(!0);try{const e=y();x||g(e);const t=await fetch(`${n}/api/personalize/translate`,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({content:e,target_language:"ur",preserve_code:!0})}),a=await t.json();m(a.translated_content);const o=document.querySelector("article");o&&(o.innerHTML=a.translated_content,o.dir="rtl"),d(!0)}catch(e){console.error("Translation failed:",e)}finally{r(!1)}},disabled:s||t,title:c?"Switch back to English":"Translate to Urdu",children:[s?"\u23f3":c?"\ud83d\udd19":"\ud83c\udf10"," ",c?"Back to English":"\u0627\u0631\u062f\u0648"]})]})]})}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var a=t(6540);const o={},l=a.createContext(o);function i(e){const n=a.useContext(l);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(l.Provider,{value:n},e.children)}}}]);